{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import pprint\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get common data list\n",
    "main_path = r'block_split_59_79.pickle'\n",
    "\n",
    "with open(main_path, 'rb') as f:\n",
    "    common_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL MAIN FUNCTIONS\n",
    "\n",
    "# ______fixing mistakes with text blocks identification (the text block is attached after the header)________________\n",
    "def drop_errors_with_blocks(common_list):\n",
    "    new_common_list = []\n",
    "    for el in common_list:\n",
    "        path, block_list = el\n",
    "        temp_list = []\n",
    "        for str_block in block_list:\n",
    "            searching = re.search(r'( +\\-{5}> +\\d+)|(\\-{5}> +\\d+)', str_block)\n",
    "            if 'HEADER' in str_block:\n",
    "                if searching:\n",
    "                    corr_list = str_block.split('----->')\n",
    "                    # print(corr_list)\n",
    "                    # print(corr_list)\n",
    "                    for temp_elem in corr_list:\n",
    "                        pattern = re.match(r'( +\\d+)', temp_elem)\n",
    "                        pattern2 = re.search(r'[а-яё]', temp_elem)\n",
    "                        if pattern:\n",
    "                            temp_list.append(f'----->{temp_elem}')\n",
    "                        elif 'HEADER' in temp_elem:\n",
    "                            temp_list.append(f'----->{temp_elem}')\n",
    "                        elif pattern2:\n",
    "                            temp_list.append(f'----->{temp_elem}')\n",
    "                        else:\n",
    "                            pass\n",
    "            else:\n",
    "                temp_list.append(str_block)\n",
    "        new_el = path, temp_list\n",
    "        new_common_list.append(new_el)\n",
    "    return new_common_list\n",
    "\n",
    "# ____________find page numbers with additional markup_________________________\n",
    "def get_pages(common_list):\n",
    "    page_list = []\n",
    "    log_list = []\n",
    "    for elem in common_list:\n",
    "        path, text_list = elem\n",
    "        searching_text = text_list[-1]\n",
    "        pattern = r'(PAGE +\\d+)'\n",
    "        try:\n",
    "            row_page = re.findall(pattern, searching_text)  \n",
    "            clean_page = row_page[-1].replace('PAGE', '').replace(' ', '')\n",
    "            tuple_pages = path, clean_page\n",
    "            page_list.append(tuple_pages)\n",
    "        except Exception as ex:\n",
    "            tuple_pages = path, 'NOT FOUND'\n",
    "            page_list.append(tuple_pages)\n",
    "    return page_list\n",
    "\n",
    "# ________fixing mistakes with indexes___________________________\n",
    "def pages_without_serias(data):\n",
    "    serial_edition_list = []\n",
    "    common_list = []\n",
    "\n",
    "    pattern = 'серий'\n",
    "\n",
    "    for el in data:\n",
    "        img_path, text = el\n",
    "        # print(type(img_path))\n",
    "        matching = re.search(pattern, img_path)\n",
    "        if matching:\n",
    "            serial_edition_list.append(el)\n",
    "        else:\n",
    "            common_list.append(el)\n",
    "    return common_list\n",
    "\n",
    "def pages_without_ukaz(data):\n",
    "    serial_edition_list = []\n",
    "    common_list = []\n",
    "\n",
    "    pattern = 'спомогательные'\n",
    "\n",
    "    for el in data:\n",
    "        img_path, text = el\n",
    "        # print(type(img_path))\n",
    "        matching = re.search(pattern, img_path)\n",
    "        if matching:\n",
    "            serial_edition_list.append(el)\n",
    "        else:\n",
    "            common_list.append(el)\n",
    "    return common_list\n",
    "\n",
    "# 'dvvu' indexes filtering\n",
    "def get_dv_vu(some_list):\n",
    "    bool_list = []\n",
    "    for f in some_list:\n",
    "        # if ',' in f:\n",
    "        if 'ДВ ВУ' in f:\n",
    "            # print(f)\n",
    "            bool_list.append('False')\n",
    "        else:\n",
    "            bool_list.append('True')\n",
    "    return bool_list\n",
    "\n",
    "# filtering of errors with headers (check registration numbers in text blocks)\n",
    "def filter_headers(common_list):\n",
    "    pattern_reg = re.compile('(\\d+-\\d+[а-я])|(\\d+-\\d+)', 0)\n",
    "    demo_2 = []\n",
    "    for el in common_list:\n",
    "        img_path, text = el\n",
    "        new_text = []\n",
    "        for block in range(len(text)):\n",
    "            if 'HEADER' in text[block] and re.search(pattern_reg, text[block]):\n",
    "                if block > 0:\n",
    "                    new_text[-1] += text[block].replace('HEADER', ' ----->')\n",
    "                else:\n",
    "                    new_text.append(text[block].replace('HEADER', ' ----->'))\n",
    "            else:\n",
    "                new_text.append(text[block])\n",
    "        el2 = img_path, new_text\n",
    "        demo_2.append(el2)\n",
    "    return demo_2\n",
    "\n",
    "\n",
    "# get list with text block and filepaths\n",
    "def get_row_uniqs(filt_header_common_list):\n",
    "    uniq_preform_list = []\n",
    "    for elem in filt_header_common_list:\n",
    "        path, text_list = elem\n",
    "        # new_text_list = []\n",
    "        for t in text_list:\n",
    "            pattern = re.compile(r'( +\\-{5}> +\\d+)|(\\-{5}> +\\d+)')\n",
    "            matching = re.match(pattern, t)\n",
    "            if 'HEADER' in t:\n",
    "                if 'См.' in t:\n",
    "                    # print(t)\n",
    "                    t = t.replace('$$$', '')\n",
    "                    useful_tuple = path, t\n",
    "                    uniq_preform_list.append(useful_tuple)\n",
    "                elif matching:\n",
    "                    # print(t)\n",
    "                    t = t.replace('$$$', '')\n",
    "                    useful_tuple = path, t\n",
    "                    uniq_preform_list.append(useful_tuple)\n",
    "                else:\n",
    "                    pass\n",
    "            if 'HEADER' not in t:\n",
    "                t = t.replace('$$$', '')\n",
    "                useful_tuple = path, t\n",
    "                uniq_preform_list.append(useful_tuple)\n",
    "    return uniq_preform_list\n",
    "\n",
    "# --------------fixing gaps --------------------\n",
    "def formater(my_sample_list):\n",
    "    temp_list = []\n",
    "    for el in my_sample_list:\n",
    "        el = list(el)\n",
    "        str_list = []\n",
    "        str_list.append(el[0])\n",
    "        el[0]=str_list\n",
    "        temp_list.append(el)\n",
    "    return(temp_list)\n",
    "\n",
    "def gap_detector(blocklist,idx):\n",
    "    counter = 1\n",
    "    text = blocklist[idx][1]\n",
    "    compiler = re.compile(r'(\\-{5}> +\\d+)|( +\\-{5}> +\\d+)')\n",
    "    pattern = re.match(compiler, text)\n",
    "    if pattern:\n",
    "        if idx+1< len(blocklist):\n",
    "            idx=idx+1\n",
    "            text = blocklist[idx][1]\n",
    "            pattern = re.match(compiler, text)\n",
    "            if not pattern:\n",
    "                if not 'HEADER' in text:\n",
    "                    trigger = detection = True\n",
    "                    counter += 1\n",
    "                    while trigger == True:\n",
    "                        if idx+counter < len(blocklist):\n",
    "                            next_text = blocklist[idx+counter-1][1]\n",
    "                            next_pattern = re.match(compiler, next_text)\n",
    "                            if not next_pattern:\n",
    "                                if not 'HEADER' in next_text:\n",
    "                                    trigger = detection = True\n",
    "                                    counter += 1\n",
    "                                else:\n",
    "                                    counter = 1 #\n",
    "                                    detection = False\n",
    "                                    return detection, counter\n",
    "                            else:\n",
    "                                trigger = False\n",
    "                                return detection, counter  \n",
    "                        else:     \n",
    "                            trigger = False  \n",
    "                            return detection, counter\n",
    "                else:\n",
    "                    counter = 1 #\n",
    "                    detection = False\n",
    "                    return detection, counter\n",
    "            else:\n",
    "                counter = 1 #\n",
    "                detection = False\n",
    "                return detection, counter\n",
    "        else:\n",
    "            counter = 1 #\n",
    "            detection = False\n",
    "            return detection, counter\n",
    "    else:\n",
    "        counter =0 #\n",
    "        detection = False\n",
    "        return detection, counter\n",
    "    \n",
    "def recollector(blocklist,idx,lenght=1):\n",
    "    *temp_my_adrr,temp_my_block = blocklist[idx]\n",
    "    temp_my_adrr = temp_my_adrr[0]\n",
    "    for idy in range(1,lenght):\n",
    "        temp_my_block = f'{temp_my_block} {blocklist[idx+idy][1]}'\n",
    "        temp_my_adrr.append(blocklist[idx+idy][0][0])\n",
    "        temp_el = [temp_my_adrr, temp_my_block]\n",
    "        blocklist[idx] = temp_el\n",
    "    return blocklist\n",
    "\n",
    "def collect_blocks_correction(collect_blocks):\n",
    "    correction_uniqs = []\n",
    "    for i in collect_blocks:\n",
    "        temp_paths = []\n",
    "        if len(i[0]) >= 1:\n",
    "            uniq_i = list(set(i[0]))\n",
    "            temp_paths.append(uniq_i)\n",
    "        else:\n",
    "            temp_paths.append(i)\n",
    "        for a in temp_paths:\n",
    "            a_paths = ', '.join(a)\n",
    "        elem = [a_paths.split(','), i[1]]\n",
    "        correction_uniqs.append(elem)\n",
    "    return correction_uniqs\n",
    "\n",
    "# fixing errors\n",
    "# delete headers which begins with number\n",
    "def check_false_headers(test_example_list):\n",
    "    data1 = test_example_list.copy() # list copy\n",
    "\n",
    "    for t in data1:\n",
    "        pattern_dig = r'(\\d+)'\n",
    "        res = re.findall(pattern_dig, t[1])\n",
    "        # print(res)\n",
    "        if len(res) < 2:\n",
    "            data1.remove(t)\n",
    "    # print(data1)\n",
    "    return data1\n",
    "\n",
    "# # TEST___________FIND MULTIPLE BLOCKS_________________________________________\n",
    "# def get_mult_num(test_string):\n",
    "#     num_list = []\n",
    "#     pattern = re.compile(r'(-{5}>  \\d+-\\d+ \\. [А-ЯЁ])|(-{5}>  \\d+-\\d+\\. [А-ЯЁ])')\n",
    "#     matching = pattern.match(test_string, 0)\n",
    "#     if matching:\n",
    "#         ord_num = re.findall(r'(\\d+-\\d+)', test_string)\n",
    "#         nums = ord_num[0].split('-')\n",
    "#         num_list.append(nums)\n",
    "#     else:\n",
    "#         pass\n",
    "#     return num_list\n",
    "\n",
    "# del additional markup\n",
    "def del_note_inf(text_list):\n",
    "    new_text = []\n",
    "    for t in text_list:\n",
    "        tt = t.replace('PAGE', '').replace('NUM', '').replace('REG', '')\n",
    "        new_text.append(tt)\n",
    "    return new_text\n",
    "\n",
    "def del_points(new_text):\n",
    "    new_text_no_points = []\n",
    "    for t in new_text:\n",
    "        if t[0] and t[-1] == '\"':\n",
    "            tt = t.replace('\"', '')\n",
    "            new_text_no_points.append(tt)\n",
    "        else:\n",
    "            new_text_no_points.append(t)\n",
    "    return new_text_no_points\n",
    "\n",
    "# GET BLOCK TYPE\n",
    "def get_block_kind(new_text_no_points):\n",
    "    kind_block_list = []\n",
    "    pattern_single = re.compile(r'(-{5}>  \\d+\\.)')\n",
    "    pattern_mult = re.compile(r'(-{5}>  \\d+-\\d+\\.)|(-{5}>  \\d+-\\d+ \\.)')\n",
    "    for n in new_text_no_points:\n",
    "        sing_match = pattern_single.match(n, 0)\n",
    "        mult_match = pattern_mult.match(n, 0)\n",
    "        if sing_match:\n",
    "            str_sing = 'single'\n",
    "            kind_block_list.append(str_sing)\n",
    "        elif mult_match:\n",
    "            str_mult = 'multiple'\n",
    "            kind_block_list.append(str_mult)\n",
    "        else:\n",
    "            str_error = 'error'\n",
    "            kind_block_list.append(str_error)\n",
    "    return kind_block_list\n",
    "\n",
    "\n",
    "# GET MULT BLOCKS\n",
    "def get_mult_num(test_string):\n",
    "    num_list = []\n",
    "    ord_num = re.findall(r'(\\d+-\\d+)', test_string)\n",
    "    nums = ord_num[0].split('-')\n",
    "    num_list.append(nums)\n",
    "    return num_list\n",
    "\n",
    "#get series of ordinal numbers\n",
    "def get_list_ords(nums):\n",
    "    first_str = nums[0][0] #string\n",
    "    sec_str = nums[0][1]\n",
    "    # first_str_idx = len(nums[0][0]) #string length\n",
    "    sec_str_idx = len(nums[0][1])\n",
    "    srez = first_str[:-sec_str_idx]\n",
    "    sec_num_str = srez + sec_str\n",
    "\n",
    "    ords_list = list(range(int(first_str), int(sec_num_str)+1))\n",
    "    return ords_list\n",
    "\n",
    "# function for sorting unique text blocks \n",
    "def get_uniqs(test_string, ords_list, uniq_list_fin = []):\n",
    "    multiple_block_str = ''\n",
    "    multiple_block_str += test_string\n",
    "    multiple_block_str += ' '\n",
    "    arrow = re.findall(r'-{5}>', multiple_block_str)\n",
    "    parts_block = multiple_block_str.split('-----> ') #the first element is empty\n",
    "    uniq_list = []\n",
    "    for part in parts_block:\n",
    "        # print(part)\n",
    "        # uniq_list = []\n",
    "        if len(arrow) - 1 == len(ords_list):\n",
    "            # print(\"YES\")\n",
    "            main_part = parts_block[1]\n",
    "            uniq = (f'-----> {main_part} {part}')\n",
    "            # print(uniq)\n",
    "            uniq_list.append(uniq)\n",
    "            # print(uniq_list)\n",
    "            uniq_list_fin = [(n, st) for n, st in zip(ords_list, uniq_list[2:])]\n",
    "\n",
    "    return uniq_list_fin\n",
    "\n",
    "# filtering 'OBU' indexes\n",
    "def get_obu(some_list):\n",
    "    bool_list = []\n",
    "    for f in some_list:\n",
    "        # if ',' in f:\n",
    "        if 'спомогательные' in f:\n",
    "            # print(f)\n",
    "            bool_list.append('False')\n",
    "        elif '/ОБУ/' in f:\n",
    "            # print(f)\n",
    "            bool_list.append('False')\n",
    "        else:\n",
    "            bool_list.append('True')\n",
    "    return bool_list\n",
    "\n",
    "# get main elements in image paths \n",
    "# sample = ['./letopisi/1979/№9/0466.jpg', \n",
    "#               './letopisi/1969/дополнительный выпуск 3/0019.jpg', \n",
    "#               './letopisi/1969/дополнительный выпуск 3/0020.jpg']\n",
    "def get_main_cut_paths(sample_two):\n",
    "    pattern_two = r'(\\/19.+\\.jpg)'\n",
    "    main_cut_paths = []\n",
    "    for s in sample_two:\n",
    "        matching_two = re.findall(pattern_two, s)\n",
    "\n",
    "        main_cut_list_paths = []\n",
    "\n",
    "        for match in matching_two:\n",
    "            cut_path_two = match.replace('№', '')\n",
    "            # print(cut_path)\n",
    "            main_cut_list_paths.append(cut_path_two[1:])\n",
    "        \n",
    "        main_cut_paths.append(main_cut_list_paths)\n",
    "    return main_cut_paths\n",
    "\n",
    "# get edition year values\n",
    "def get_year(some_list):\n",
    "    list_year = []\n",
    "    pattern_year = r'(19\\d+)'\n",
    "    # find = re.findall()\n",
    "    for i in some_list:\n",
    "        for year in i:\n",
    "            find = re.findall(pattern_year, year)\n",
    "            list_year.append(find[0])\n",
    "    return list_year\n",
    "\n",
    "# get edition number\n",
    "error_list = []\n",
    "def get_edition(path_list):\n",
    "    my_edition = []\n",
    "    some_eds = []\n",
    "    pattern = r'(\\d+)'\n",
    "    for p in path_list:\n",
    "        temp_list = p.split('/')\n",
    "        elem_ed = temp_list[-2]\n",
    "        edition = re.search(pattern, elem_ed)\n",
    "        if edition:\n",
    "            some_eds.append(edition.group(0))\n",
    "        else:\n",
    "            # print(p)\n",
    "            error_list.append(p)\n",
    "    if len(some_eds) >= 1:\n",
    "        main_numb = some_eds[0]\n",
    "        my_edition.append(main_numb)\n",
    "    else:\n",
    "        error_list.append(p)\n",
    "    a = ''.join(my_edition)\n",
    "    if a.isdigit():\n",
    "        return int(a)\n",
    "\n",
    "# get number of jpeg from image paths\n",
    "error_jpg_list = []\n",
    "def get_jpg_num(path_list):\n",
    "    some_jpg = []\n",
    "    pattern = r'(\\d+\\.jpg)'\n",
    "    for p in path_list:\n",
    "        temp_list = p.split('/')\n",
    "        elem_ed = temp_list[-1]\n",
    "        edition = re.search(pattern, elem_ed)\n",
    "        if edition:\n",
    "            some_jpg.append(edition.group(0).replace('.jpg', ''))\n",
    "        else:\n",
    "            # print(p)\n",
    "            error_jpg_list.append(p)\n",
    "    return list(some_jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_list = get_pages(common_list) #get page number from every scan\n",
    "\n",
    "# delete auxiliary indexes and serial indexes + erroneous headers\n",
    "common_list_2 = pages_without_serias(page_list)\n",
    "common_list_3 = pages_without_ukaz(common_list_2)\n",
    "filt_header_common_list = filter_headers(common_list_3)\n",
    "# unique text blocks list\n",
    "uniq_preform_list = get_row_uniqs(filt_header_common_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN CIRCLE\n",
    "# fixing of gaps in text blocks in cases where text block is on page breaks and get unique text blocks\n",
    "start_time = time.time()\n",
    "poplist=[]\n",
    "collect_blocks=[]\n",
    "try:       \n",
    "    t_list = formater(uniq_preform_list)\n",
    "    for idx, block in enumerate(t_list):\n",
    "    # print(idx, block)\n",
    "        *isgap,lenght = gap_detector(t_list,idx)\n",
    "    # print(isgap, lenght)\n",
    "        if isgap[0]:\n",
    "            collect_blocks = recollector(t_list,idx, lenght)\n",
    "    # print(collect_blocks)\n",
    "        if isgap[0] == False and lenght==0: \n",
    "            poplist.append(idx)\n",
    "    poplist = sorted(poplist, reverse=True)\n",
    "    # print(poplist)\n",
    "    for el in poplist:\n",
    "        collect_blocks.pop(el)\n",
    "except Exception as e:\n",
    "    with open('logs_gaps.log', 'a', encoding = 'utf8') as f:\n",
    "        f.write(f'{e}')\n",
    "#         pass\n",
    "end_time = time.time()\n",
    "\n",
    "circle_working_time = end_time - start_time\n",
    "print(circle_working_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get row data with unique text blocks\n",
    "test = collect_blocks_correction(collect_blocks)\n",
    "# delete headers which begins with figures\n",
    "data1 = check_false_headers(test)\n",
    "\n",
    "# get dataframe with paths and text blocks\n",
    "df = pd.DataFrame(data1, columns = ['paths', 'blocktext'])\n",
    "df.paths = df.paths.apply(tuple) #conversion to common type of data\n",
    "# create the second dataframe with pages on scans\n",
    "page_df = pd.DataFrame(page_list, columns = ['paths', 'num_of_page'])\n",
    "\n",
    "# merger of dataframe where every text block linked with scan of Book Chronicles\n",
    "df = df.reset_index(names = 'block_id')\n",
    "df= df.explode('paths')\n",
    "df = df.reset_index(drop = True)\n",
    "df.paths = df.paths.str.strip()\n",
    "merged_df = df.merge(page_df, how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correction of dataframe formating\n",
    "merged_df.paths = merged_df.paths.apply(lambda x: x +', ')\n",
    "merged_df.num_of_page = merged_df.num_of_page.apply(lambda x: x +', ')\n",
    "new_df = merged_df.groupby('block_id').agg({'paths':'sum','num_of_page':'sum','blocktext':'first'}).reset_index()\n",
    "new_df.num_of_page = new_df.num_of_page.str.strip(', ')\n",
    "new_df.paths = new_df.paths.str.strip(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do text correction\n",
    "text_list = new_df['blocktext'].to_list()\n",
    "\n",
    "# MAIN FOR FINAL COMMON DATAFRAME _______________________________________________________\n",
    "no_note = del_note_inf(text_list) # delete words from additional markup\n",
    "no_points = del_points(no_note) # delete quotation marks\n",
    "list_block_kinds = get_block_kind(no_points) # get block type to list\n",
    "# circle for assembly of multiple text blocks\n",
    "final_block_list = []\n",
    "for one_text, kind_block in zip(no_points, list_block_kinds):\n",
    "    if kind_block == 'multiple':\n",
    "        nums = get_mult_num(one_text)\n",
    "        order_nums = get_list_ords(nums)\n",
    "        uniqs_from_multi = get_uniqs(one_text, order_nums)\n",
    "        final_block_list.append(uniqs_from_multi)\n",
    "    else:\n",
    "        final_block_list.append(one_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# добавочный датафрейм сливаем с основным\n",
    "add_dict = {'block_kind': list_block_kinds, 'final_blocks': final_block_list}\n",
    "\n",
    "add_df = pd.DataFrame(add_dict)\n",
    "\n",
    "final_df = new_df.join(add_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working with dataframe and filtering paths\n",
    "first = final_df['paths'].tolist()\n",
    "bool_first = get_obu(first)\n",
    "final_df['not_obu'] = bool_first\n",
    "main_df = final_df.loc[final_df['not_obu'] != 'False']\n",
    "main_df = main_df.reset_index()\n",
    "del main_df['block_id']\n",
    "\n",
    "sample_two = main_df['paths'].tolist()\n",
    "main_cut_paths = get_main_cut_paths(sample_two)\n",
    "main_df['cut_paths'] = main_cut_paths\n",
    "del main_df['index']\n",
    "del main_df['level_0']\n",
    "del main_df['not_obu']\n",
    "del main_df['block_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get years of issue\n",
    "list_for_years = main_df['cut_paths'].tolist()\n",
    "sample_year = get_year(list_for_years)\n",
    "main_df['year'] = sample_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get editions from image paths\n",
    "main_df['num_edition'] = main_df['cut_paths'].apply(get_edition)\n",
    "# print(error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get jpeg-file number of image\n",
    "main_df['jpg_num'] = main_df['cut_paths'].apply(get_jpg_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST\n",
    "# # check pass values\n",
    "# print(set(main_df['num_edition'].isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting of dataframe by chronology\n",
    "d = main_df[main_df.isna().sum(axis=1).eq(1)]\n",
    "df_clear = main_df.dropna().reset_index()\n",
    "sort_df = df_clear.sort_values(['year', 'num_edition'])\n",
    "sort_csv_df = sort_df.reset_index()\n",
    "del sort_csv_df['level_0']\n",
    "del sort_csv_df['edition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering 'dvvu' pointers in paths\n",
    "add_bool_list = sort_csv_df['paths'].tolist()\n",
    "res = get_dv_vu(add_bool_list)\n",
    "sort_csv_df['bool_dvvu'] = res\n",
    "new_df = sort_csv_df.drop(sort_csv_df.loc[sort_csv_df['bool_dvvu'] == 'False'].index)\n",
    "new_df = new_df.reset_index()\n",
    "del new_df['index']\n",
    "del new_df['bool_dvvu']\n",
    "del new_df['level_0']\n",
    "new_df['jpgs'] = new_df['cut_paths'].apply(get_jpg_num)\n",
    "# save result\n",
    "# new_df.to_csv('sort_df_all.csv', sep = ';', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST\n",
    "# # when you need only single text blocks\n",
    "# only_single = new_df.loc[new_df['block_kind'] == 'single']\n",
    "# del only_single['jpgs']\n",
    "# only_single.reset_index()\n",
    "# only_single.shape\n",
    "\n",
    "# # when you need only multiple text blocks\n",
    "# only_multiple = new_df.loc[new_df['block_kind'] == 'multiple']\n",
    "# multiple_uniqs = only_multiple.loc[only_multiple['final_blocks'] != '[]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST\n",
    "# # check and count of multiple text blocks in dataframe\n",
    "\n",
    "# checking = final_df['final_blocks'].to_list()\n",
    "# count_mult_blocks = []\n",
    "# for block in checking:\n",
    "#     try:\n",
    "#         if type(block) == list:\n",
    "#             # print(block)\n",
    "#             len_block = len(block)\n",
    "#             count_mult_blocks.append(len_block)\n",
    "#     except:\n",
    "#         pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST\n",
    "# # STATISTICS__________________\n",
    "# # count of unique text blocks in multiple blocks\n",
    "# nul_blocks = set(count_mult_blocks)\n",
    "# # added multiple blocks\n",
    "# sum_mult_blocks = sum(count_mult_blocks)\n",
    "# # count multiple blocks (value 0)\n",
    "# c = Counter(count_mult_blocks)\n",
    "# # count different text block's types\n",
    "# checking_error = final_df['block_kind'].to_list()\n",
    "# c_error = Counter(checking_error)\n",
    "# print(c_error)\n",
    "# print(c)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
